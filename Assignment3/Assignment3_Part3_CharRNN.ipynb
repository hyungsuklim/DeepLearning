{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 3: Language Modeling with CharRNN\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sang-gil Lee, October 2018\n",
    "\n",
    "This is a character-level language model using recurrent neural networks (RNNs).\n",
    "It has become very popular as a starter kit for learning how RNN works in practice.\n",
    "\n",
    "Before we start, what is \"language modeling\" anyway? Intuitively, \"language modeling\" is teaching the model about a general probability distribution of our words and sentences.\n",
    "\n",
    "So we ask the model like: \"hey just say whatever words from your estimation of the wikipedia word distribution\", and the model responds like \"ok, i learned from wikipedia, and the most frequent word is \"the\". so let me start with \"the\". the wikipedia is blah blah blah\"\n",
    "\n",
    "Thus, by teaching the model to speak for itself, we can test the model's capability of learning temporal relationships between sequences.\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/karpathy/char-rnn\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "But the original code is written in lua torch which looks less pretty :(\n",
    "\n",
    "There is a clean port of char-RNN in TensorFlow\n",
    "https://github.com/sherjilozair/char-rnn-tensorflow\n",
    "This iPython notebook is basically a copypasta of this repo.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "### AND MOST IMPORTANTLY, IF YOU JUST BLINDLY COPY PASTE THE CODE, YOU SHALL RUIN YOUR EXAM.\n",
    "### The exam is designed to be solvable for students that actually have written the code themselves.\n",
    "At least strictly re-type the codes from the original repo line-by-line, and understand what each line means thoroughly.\n",
    "\n",
    "## YOU HAVE BEEN WARNED. :)\n",
    "\n",
    "\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1, 2 & 3**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Character language modeling (40 points)\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. implementing **1. \\_\\_init\\_\\_()** and **2. sample()** of RNN **Model()** class from **char_rnn.py**\n",
    "\n",
    "2. briefly summarizing, at the end of the script, how you implmeneted the model & why you changed some other parts of the code. yes,  <font color=red> there are other intentional pitfalls inside the code </font>. just copy-pasting the \\_\\_init\\_\\_() will not work. can you tell me why?\n",
    "\n",
    "### The Definition of **\"work\"** is as follows:\n",
    "\n",
    "1. Training loss must be <font color=red> below 0.2 </font>. We will check the training loss output from the training code block. We don't split the data into train-valid-test. Don't forget to <font color=red> NOT clear the output from train(args)</font>, where the training loss will be printed! TA will check the logged output from train(args)\n",
    "\n",
    "2. calling sample(args.sample) at the last code block <font color=red> must generate some meaningful sentences </font>. The quality of the sentence does not count, unless the generated sentence is something like \"aaaaaaaaaaaaaabbbbbb\" or \"b\" u tlttfcwaU c  fGcnrh i.\\nh mt he!bsthpme\".\n",
    "\n",
    "\n",
    "\n",
    "Now proceed to the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "# ipython magic function for limiting the gpu to be seen for tensorflow\n",
    "# if you have just 1 GPU, specify the value to 0\n",
    "# if you have multiple GPUs (nut) and want to specify which GPU to use, specify this value to 0 or 1 or etc.\n",
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for TensorFlow vram efficiency: if this is not specified, the model hogs all the VRAM even if it's not necessary\n",
    "# bad & greedy TF! but it has a reason for this design choice FWIW, try googling it if interested\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=5, data_dir='data/tinyshakespeare', decay_rate=0.97, grad_clip=20.0, init_from=None, input_keep_prob=1.0, learning_rate=0.002, model='lstm', num_epochs=50, num_layers=5, output_keep_prob=1.0, rnn_size=128, save_dir='models_char_rnn', save_every=1000, seq_length=500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing\n",
    "parser = argparse.ArgumentParser(\n",
    "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',\n",
    "                    help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='directory to store checkpointed models')\n",
    "parser.add_argument('--save_every', type=int, default=1000,\n",
    "                    help='Save frequency. Number of passes between checkpoints of the model.')\n",
    "parser.add_argument('--init_from', type=str, default=None,\n",
    "                    help=\"\"\"continue training from saved model at this path (usually \"save\").\n",
    "                        Path must contain files saved by previous training process:\n",
    "                        'config.pkl'        : configuration;\n",
    "                        'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                        'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                              Note: this file contains absolute paths, be careful when moving files around;\n",
    "                        'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                         Model params must be the same between multiple runs (model, rnn_size, num_layers and seq_length).\n",
    "                    \"\"\")\n",
    "# Model params\n",
    "parser.add_argument('--model', type=str, default='lstm',\n",
    "                    help='lstm, rnn, gru, or nas')\n",
    "parser.add_argument('--rnn_size', type=int, default=128,\n",
    "                    help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=5,\n",
    "                    help='number of layers in the RNN')\n",
    "# Optimization\n",
    "parser.add_argument('--seq_length', type=int, default=500,\n",
    "                    help='RNN sequence length. Number of timesteps to unroll for.')\n",
    "parser.add_argument('--batch_size', type=int, default=5,\n",
    "                    help=\"\"\"minibatch size. Number of sequences propagated through the network in parallel.\n",
    "                            Pick batch-sizes to fully leverage the GPU (e.g. until the memory is filled up)\n",
    "                            commonly in the range 10-500.\"\"\")\n",
    "parser.add_argument('--num_epochs', type=int, default=50,\n",
    "                    help='number of epochs. Number of full passes through the training examples.')\n",
    "parser.add_argument('--grad_clip', type=float, default=20.,\n",
    "                    help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.002,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.97,\n",
    "                    help='decay rate for rmsprop')\n",
    "parser.add_argument('--output_keep_prob', type=float, default=1.0,\n",
    "                    help='probability of keeping weights in the hidden layer')\n",
    "parser.add_argument('--input_keep_prob', type=float, default=1.0,\n",
    "                    help='probability of keeping weights in the input layer')\n",
    "\n",
    "# needed for argparsing within jupyter notebook\n",
    "# https://stackoverflow.com/questions/30656777/how-to-call-module-written-with-argparse-in-ipython-notebook\n",
    "sys.argv = ['-f']\n",
    "args = parser.parse_args()\n",
    "\n",
    "# print args: see if the hyperparemeters look pretty to you\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "[[49  9  7 ...  3  3  7]\n",
      " [11  1  6 ... 15  0 17]\n",
      " [26  0 19 ... 18  0  2]\n",
      " [ 8  0  3 ...  2  0  6]\n",
      " [18  9  2 ... 21  0  5]]\n",
      "(5, 500)\n",
      "[[ 9  7  6 ...  3  7  0]\n",
      " [ 1  6  0 ...  0 17  1]\n",
      " [ 0 19  7 ...  0  2  3]\n",
      " [ 0  3 13 ...  0  6  4]\n",
      " [ 9  2  9 ...  0  5  4]]\n",
      "(5, 500)\n"
     ]
    }
   ],
   "source": [
    "# protip: always check the data and poke around the data yourself\n",
    "# you will get a lot of insights by looking at the data\n",
    "data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "data_loader.reset_batch_pointer()\n",
    "\n",
    "x, y = data_loader.next_batch()\n",
    "\n",
    "# our data has a shape of (N, T), where N=batch_size and T=seq_length\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1\n",
      "  0 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1\n",
      "  4  7  0 14  1  0  6 23  1  4 28 25 10 10 26 11 11 24 10 35 23  1  4 28\n",
      " 16  0  6 23  1  4 28 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24\n",
      " 10 50  3 13  0  4  7  1  0  4 11 11  0  7  1  6  3 11 27  1 12  0  7  4\n",
      "  2  5  1  7  0  2  3  0 12  9  1  0  2  5  4  8  0  2  3  0 18  4 14  9\n",
      "  6  5 44 10 10 26 11 11 24 10 34  1  6  3 11 27  1 12 25  0  7  1  6  3\n",
      " 11 27  1 12 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 49  9\n",
      "  7  6  2 16  0 15  3 13  0 28  8  3 17  0 37  4  9 13  6  0 42  4  7 19\n",
      "  9 13  6  0  9  6  0 19  5  9  1 18  0  1  8  1 14 15  0  2  3  0  2  5\n",
      "  1  0 23  1  3 23 11  1 25 10 10 26 11 11 24 10 39  1  0 28  8  3 17 30\n",
      "  2 16  0 17  1  0 28  8  3 17 30  2 25 10 10 49  9  7  6  2  0 37  9  2\n",
      "  9 57  1  8 24 10 36  1  2  0 13  6  0 28  9 11 11  0  5  9 14 16  0  4\n",
      "  8 12  0 17  1 30 11 11  0  5  4 27  1  0 19  3  7  8  0  4  2  0  3 13\n",
      "  7  0  3 17  8  0 23  7  9 19  1 25 10 21  6 30  2  0  4  0 27  1  7 12\n",
      "  9 19  2 44 10 10 26 11 11 24 10 33  3  0 14  3  7  1  0  2  4 11 28  9\n",
      "  8 20  0  3  8 30  2 38  0 11  1  2  0  9  2  0 22  1  0 12  3  8  1 24\n",
      "  0  4 17  4 15 16  0  4 17  4 15 46 10 10 35  1 19  3  8 12  0 37  9  2\n",
      "  9 57  1  8 24 10 32  8  1  0 17  3  7 12 16  0 20  3  3 12  0 19  9  2\n",
      "  9 57  1  8  6 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 39\n",
      "  1  0  4  7  1  0  4 19 19  3 13  8  2  1 12  0 23  3  3  7]\n",
      "[ 9  7  6  2  0 37  9  2  9 57  1  8 24 10 43  1 18  3  7  1  0 17  1  0\n",
      " 23  7  3 19  1  1 12  0  4  8 15  0 18 13  7  2  5  1  7 16  0  5  1  4\n",
      "  7  0 14  1  0  6 23  1  4 28 25 10 10 26 11 11 24 10 35 23  1  4 28 16\n",
      "  0  6 23  1  4 28 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10\n",
      " 50  3 13  0  4  7  1  0  4 11 11  0  7  1  6  3 11 27  1 12  0  7  4  2\n",
      "  5  1  7  0  2  3  0 12  9  1  0  2  5  4  8  0  2  3  0 18  4 14  9  6\n",
      "  5 44 10 10 26 11 11 24 10 34  1  6  3 11 27  1 12 25  0  7  1  6  3 11\n",
      " 27  1 12 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 49  9  7\n",
      "  6  2 16  0 15  3 13  0 28  8  3 17  0 37  4  9 13  6  0 42  4  7 19  9\n",
      " 13  6  0  9  6  0 19  5  9  1 18  0  1  8  1 14 15  0  2  3  0  2  5  1\n",
      "  0 23  1  3 23 11  1 25 10 10 26 11 11 24 10 39  1  0 28  8  3 17 30  2\n",
      " 16  0 17  1  0 28  8  3 17 30  2 25 10 10 49  9  7  6  2  0 37  9  2  9\n",
      " 57  1  8 24 10 36  1  2  0 13  6  0 28  9 11 11  0  5  9 14 16  0  4  8\n",
      " 12  0 17  1 30 11 11  0  5  4 27  1  0 19  3  7  8  0  4  2  0  3 13  7\n",
      "  0  3 17  8  0 23  7  9 19  1 25 10 21  6 30  2  0  4  0 27  1  7 12  9\n",
      " 19  2 44 10 10 26 11 11 24 10 33  3  0 14  3  7  1  0  2  4 11 28  9  8\n",
      " 20  0  3  8 30  2 38  0 11  1  2  0  9  2  0 22  1  0 12  3  8  1 24  0\n",
      "  4 17  4 15 16  0  4 17  4 15 46 10 10 35  1 19  3  8 12  0 37  9  2  9\n",
      " 57  1  8 24 10 32  8  1  0 17  3  7 12 16  0 20  3  3 12  0 19  9  2  9\n",
      " 57  1  8  6 25 10 10 49  9  7  6  2  0 37  9  2  9 57  1  8 24 10 39  1\n",
      "  0  4  7  1  0  4 19 19  3 13  8  2  1 12  0 23  3  3  7  0]\n"
     ]
    }
   ],
   "source": [
    "# see what the first entry of the batch look like\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "# y is just an x shifted to the left by one: so the network will predict the next token y given x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop definition\n",
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "    print(\"vocabulary size: \" + str(args.vocab_size))\n",
    "\n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        # check if all necessary files exist\n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.latest_checkpoint(args.init_from)\n",
    "        assert ckpt, \"No checkpoint found\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same = [\"model\", \"rnn_size\", \"num_layers\", \"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "\n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagree on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagree on dictionary mappings!\"\n",
    "\n",
    "    if not os.path.isdir(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "    \n",
    "    print(\"building the model... may take some time...\")\n",
    "    ##################### This line builds the CharRNN model defined in char_rnn.py #####################\n",
    "    model = Model(args)\n",
    "    print(\"model built! starting training...\")\n",
    "\n",
    "    # training\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt)\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr,\n",
    "                               args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state)\n",
    "            \n",
    "            for b in range(int(data_loader.num_batches)):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                for i, (c, h) in enumerate(model.initial_state):\n",
    "                    feed[c] = state[i].c\n",
    "                    feed[h] = state[i].h\n",
    "\n",
    "                train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "\n",
    "                end = time.time()\n",
    "                \n",
    "                # print training log every 100 steps\n",
    "                if ((e * data_loader.num_batches + b) % 100 == 0):\n",
    "                    print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                          .format(e * data_loader.num_batches + b,\n",
    "                                  args.num_epochs * data_loader.num_batches,\n",
    "                                  e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                        or (e == args.num_epochs-1 and\n",
    "                            b == data_loader.num_batches-1):\n",
    "                    # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path,\n",
    "                               global_step=e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "building the model... may take some time...\n",
      "model built! starting training...\n",
      "0/22300 (epoch 0), train_loss = 4.175, time/batch = 7.032\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "100/22300 (epoch 0), train_loss = 3.340, time/batch = 2.028\n",
      "200/22300 (epoch 0), train_loss = 3.291, time/batch = 1.958\n",
      "300/22300 (epoch 0), train_loss = 3.115, time/batch = 2.003\n",
      "400/22300 (epoch 0), train_loss = 2.568, time/batch = 1.959\n",
      "500/22300 (epoch 1), train_loss = 2.410, time/batch = 2.034\n",
      "600/22300 (epoch 1), train_loss = 2.314, time/batch = 2.094\n",
      "700/22300 (epoch 1), train_loss = 2.197, time/batch = 2.172\n",
      "800/22300 (epoch 1), train_loss = 2.178, time/batch = 2.125\n",
      "900/22300 (epoch 2), train_loss = 2.126, time/batch = 2.016\n",
      "1000/22300 (epoch 2), train_loss = 2.019, time/batch = 1.982\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "1100/22300 (epoch 2), train_loss = 2.063, time/batch = 2.062\n",
      "1200/22300 (epoch 2), train_loss = 2.039, time/batch = 2.172\n",
      "1300/22300 (epoch 2), train_loss = 1.992, time/batch = 2.200\n",
      "1400/22300 (epoch 3), train_loss = 1.919, time/batch = 2.023\n",
      "1500/22300 (epoch 3), train_loss = 1.999, time/batch = 2.041\n",
      "1600/22300 (epoch 3), train_loss = 1.946, time/batch = 0.357\n",
      "1700/22300 (epoch 3), train_loss = 1.871, time/batch = 0.360\n",
      "1800/22300 (epoch 4), train_loss = 1.973, time/batch = 0.357\n",
      "1900/22300 (epoch 4), train_loss = 1.871, time/batch = 0.351\n",
      "2000/22300 (epoch 4), train_loss = 1.913, time/batch = 0.354\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "2100/22300 (epoch 4), train_loss = 1.910, time/batch = 0.355\n",
      "2200/22300 (epoch 4), train_loss = 1.830, time/batch = 0.351\n",
      "2300/22300 (epoch 5), train_loss = 1.737, time/batch = 0.357\n",
      "2400/22300 (epoch 5), train_loss = 1.927, time/batch = 0.354\n",
      "2500/22300 (epoch 5), train_loss = 1.867, time/batch = 0.358\n",
      "2600/22300 (epoch 5), train_loss = 1.832, time/batch = 0.354\n",
      "2700/22300 (epoch 6), train_loss = 1.786, time/batch = 0.356\n",
      "2800/22300 (epoch 6), train_loss = 1.757, time/batch = 0.359\n",
      "2900/22300 (epoch 6), train_loss = 1.759, time/batch = 0.359\n",
      "3000/22300 (epoch 6), train_loss = 1.824, time/batch = 0.357\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "3100/22300 (epoch 6), train_loss = 1.759, time/batch = 0.352\n",
      "3200/22300 (epoch 7), train_loss = 1.794, time/batch = 0.356\n",
      "3300/22300 (epoch 7), train_loss = 1.723, time/batch = 0.352\n",
      "3400/22300 (epoch 7), train_loss = 1.742, time/batch = 0.358\n",
      "3500/22300 (epoch 7), train_loss = 1.777, time/batch = 0.354\n",
      "3600/22300 (epoch 8), train_loss = 1.714, time/batch = 0.359\n",
      "3700/22300 (epoch 8), train_loss = 1.757, time/batch = 0.451\n",
      "3800/22300 (epoch 8), train_loss = 1.688, time/batch = 0.447\n",
      "3900/22300 (epoch 8), train_loss = 1.714, time/batch = 0.453\n",
      "4000/22300 (epoch 8), train_loss = 1.600, time/batch = 0.357\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "4100/22300 (epoch 9), train_loss = 1.642, time/batch = 0.356\n",
      "4200/22300 (epoch 9), train_loss = 1.669, time/batch = 0.361\n",
      "4300/22300 (epoch 9), train_loss = 1.663, time/batch = 0.357\n",
      "4400/22300 (epoch 9), train_loss = 1.610, time/batch = 0.353\n",
      "4500/22300 (epoch 10), train_loss = 1.685, time/batch = 0.358\n",
      "4600/22300 (epoch 10), train_loss = 1.647, time/batch = 0.356\n",
      "4700/22300 (epoch 10), train_loss = 1.607, time/batch = 0.354\n",
      "4800/22300 (epoch 10), train_loss = 1.496, time/batch = 0.354\n",
      "4900/22300 (epoch 10), train_loss = 1.568, time/batch = 0.352\n",
      "5000/22300 (epoch 11), train_loss = 1.524, time/batch = 0.356\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "5100/22300 (epoch 11), train_loss = 1.626, time/batch = 0.354\n",
      "5200/22300 (epoch 11), train_loss = 1.572, time/batch = 0.355\n",
      "5300/22300 (epoch 11), train_loss = 1.510, time/batch = 0.355\n",
      "5400/22300 (epoch 12), train_loss = 1.675, time/batch = 0.354\n",
      "5500/22300 (epoch 12), train_loss = 1.626, time/batch = 0.355\n",
      "5600/22300 (epoch 12), train_loss = 1.635, time/batch = 0.354\n",
      "5700/22300 (epoch 12), train_loss = 1.435, time/batch = 0.357\n",
      "5800/22300 (epoch 13), train_loss = 1.508, time/batch = 0.360\n",
      "5900/22300 (epoch 13), train_loss = 1.592, time/batch = 0.351\n",
      "6000/22300 (epoch 13), train_loss = 1.579, time/batch = 0.358\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "6100/22300 (epoch 13), train_loss = 1.514, time/batch = 0.356\n",
      "6200/22300 (epoch 13), train_loss = 1.546, time/batch = 0.357\n",
      "6300/22300 (epoch 14), train_loss = 1.531, time/batch = 0.356\n",
      "6400/22300 (epoch 14), train_loss = 1.567, time/batch = 0.354\n",
      "6500/22300 (epoch 14), train_loss = 1.579, time/batch = 0.350\n",
      "6600/22300 (epoch 14), train_loss = 1.453, time/batch = 0.354\n",
      "6700/22300 (epoch 15), train_loss = 1.451, time/batch = 0.358\n",
      "6800/22300 (epoch 15), train_loss = 1.546, time/batch = 0.354\n",
      "6900/22300 (epoch 15), train_loss = 1.535, time/batch = 0.354\n",
      "7000/22300 (epoch 15), train_loss = 1.613, time/batch = 0.354\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "7100/22300 (epoch 15), train_loss = 1.543, time/batch = 0.354\n",
      "7200/22300 (epoch 16), train_loss = 1.570, time/batch = 0.363\n",
      "7300/22300 (epoch 16), train_loss = 1.571, time/batch = 0.355\n",
      "7400/22300 (epoch 16), train_loss = 1.476, time/batch = 0.355\n",
      "7500/22300 (epoch 16), train_loss = 1.522, time/batch = 0.354\n",
      "7600/22300 (epoch 17), train_loss = 1.540, time/batch = 0.353\n",
      "7700/22300 (epoch 17), train_loss = 1.473, time/batch = 0.356\n",
      "7800/22300 (epoch 17), train_loss = 1.544, time/batch = 0.355\n",
      "7900/22300 (epoch 17), train_loss = 1.489, time/batch = 0.360\n",
      "8000/22300 (epoch 17), train_loss = 1.428, time/batch = 0.358\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "8100/22300 (epoch 18), train_loss = 1.506, time/batch = 0.357\n",
      "8200/22300 (epoch 18), train_loss = 1.524, time/batch = 0.356\n",
      "8300/22300 (epoch 18), train_loss = 1.410, time/batch = 0.351\n",
      "8400/22300 (epoch 18), train_loss = 1.492, time/batch = 0.356\n",
      "8500/22300 (epoch 19), train_loss = 1.501, time/batch = 0.355\n",
      "8600/22300 (epoch 19), train_loss = 1.374, time/batch = 0.359\n",
      "8700/22300 (epoch 19), train_loss = 1.508, time/batch = 0.353\n",
      "8800/22300 (epoch 19), train_loss = 1.504, time/batch = 0.364\n",
      "8900/22300 (epoch 19), train_loss = 1.437, time/batch = 0.358\n",
      "9000/22300 (epoch 20), train_loss = 1.611, time/batch = 0.357\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "9100/22300 (epoch 20), train_loss = 1.494, time/batch = 0.355\n",
      "9200/22300 (epoch 20), train_loss = 1.520, time/batch = 0.357\n",
      "9300/22300 (epoch 20), train_loss = 1.526, time/batch = 0.358\n",
      "9400/22300 (epoch 21), train_loss = 1.526, time/batch = 0.356\n",
      "9500/22300 (epoch 21), train_loss = 1.411, time/batch = 0.361\n",
      "9600/22300 (epoch 21), train_loss = 1.511, time/batch = 0.354\n",
      "9700/22300 (epoch 21), train_loss = 1.438, time/batch = 0.354\n",
      "9800/22300 (epoch 21), train_loss = 1.420, time/batch = 0.355\n",
      "9900/22300 (epoch 22), train_loss = 1.466, time/batch = 0.355\n",
      "10000/22300 (epoch 22), train_loss = 1.468, time/batch = 0.361\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "10100/22300 (epoch 22), train_loss = 1.479, time/batch = 0.357\n",
      "10200/22300 (epoch 22), train_loss = 1.428, time/batch = 0.358\n",
      "10300/22300 (epoch 23), train_loss = 1.432, time/batch = 0.353\n",
      "10400/22300 (epoch 23), train_loss = 1.437, time/batch = 0.355\n",
      "10500/22300 (epoch 23), train_loss = 1.467, time/batch = 0.354\n",
      "10600/22300 (epoch 23), train_loss = 1.370, time/batch = 0.356\n",
      "10700/22300 (epoch 23), train_loss = 1.342, time/batch = 0.354\n",
      "10800/22300 (epoch 24), train_loss = 1.540, time/batch = 0.355\n",
      "10900/22300 (epoch 24), train_loss = 1.437, time/batch = 0.355\n",
      "11000/22300 (epoch 24), train_loss = 1.576, time/batch = 0.357\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "11100/22300 (epoch 24), train_loss = 1.387, time/batch = 0.358\n",
      "11200/22300 (epoch 25), train_loss = 1.465, time/batch = 0.359\n",
      "11300/22300 (epoch 25), train_loss = 1.465, time/batch = 0.355\n",
      "11400/22300 (epoch 25), train_loss = 1.508, time/batch = 0.353\n",
      "11500/22300 (epoch 25), train_loss = 1.402, time/batch = 0.355\n",
      "11600/22300 (epoch 26), train_loss = 1.416, time/batch = 0.353\n",
      "11700/22300 (epoch 26), train_loss = 1.531, time/batch = 0.354\n",
      "11800/22300 (epoch 26), train_loss = 1.411, time/batch = 0.357\n",
      "11900/22300 (epoch 26), train_loss = 1.466, time/batch = 0.352\n",
      "12000/22300 (epoch 26), train_loss = 1.413, time/batch = 0.356\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "12100/22300 (epoch 27), train_loss = 1.477, time/batch = 0.357\n",
      "12200/22300 (epoch 27), train_loss = 1.463, time/batch = 0.354\n",
      "12300/22300 (epoch 27), train_loss = 1.370, time/batch = 0.356\n",
      "12400/22300 (epoch 27), train_loss = 1.393, time/batch = 0.359\n",
      "12500/22300 (epoch 28), train_loss = 1.409, time/batch = 0.356\n",
      "12600/22300 (epoch 28), train_loss = 1.476, time/batch = 0.360\n",
      "12700/22300 (epoch 28), train_loss = 1.405, time/batch = 0.354\n",
      "12800/22300 (epoch 28), train_loss = 1.403, time/batch = 0.357\n",
      "12900/22300 (epoch 28), train_loss = 1.359, time/batch = 0.357\n",
      "13000/22300 (epoch 29), train_loss = 1.405, time/batch = 0.353\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "13100/22300 (epoch 29), train_loss = 1.499, time/batch = 0.358\n",
      "13200/22300 (epoch 29), train_loss = 1.401, time/batch = 0.360\n",
      "13300/22300 (epoch 29), train_loss = 1.344, time/batch = 0.354\n",
      "13400/22300 (epoch 30), train_loss = 1.428, time/batch = 0.359\n",
      "13500/22300 (epoch 30), train_loss = 1.350, time/batch = 0.356\n",
      "13600/22300 (epoch 30), train_loss = 1.422, time/batch = 0.355\n",
      "13700/22300 (epoch 30), train_loss = 1.423, time/batch = 0.353\n",
      "13800/22300 (epoch 30), train_loss = 1.411, time/batch = 0.353\n",
      "13900/22300 (epoch 31), train_loss = 1.443, time/batch = 0.362\n",
      "14000/22300 (epoch 31), train_loss = 1.458, time/batch = 0.359\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "14100/22300 (epoch 31), train_loss = 1.378, time/batch = 0.358\n",
      "14200/22300 (epoch 31), train_loss = 1.441, time/batch = 0.354\n",
      "14300/22300 (epoch 32), train_loss = 1.463, time/batch = 0.357\n",
      "14400/22300 (epoch 32), train_loss = 1.293, time/batch = 0.357\n",
      "14500/22300 (epoch 32), train_loss = 1.482, time/batch = 0.359\n",
      "14600/22300 (epoch 32), train_loss = 1.440, time/batch = 0.354\n",
      "14700/22300 (epoch 32), train_loss = 1.421, time/batch = 0.357\n",
      "14800/22300 (epoch 33), train_loss = 1.358, time/batch = 0.358\n",
      "14900/22300 (epoch 33), train_loss = 1.475, time/batch = 0.359\n",
      "15000/22300 (epoch 33), train_loss = 1.334, time/batch = 0.355\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "15100/22300 (epoch 33), train_loss = 1.427, time/batch = 0.357\n",
      "15200/22300 (epoch 34), train_loss = 1.411, time/batch = 0.352\n",
      "15300/22300 (epoch 34), train_loss = 1.319, time/batch = 0.361\n",
      "15400/22300 (epoch 34), train_loss = 1.413, time/batch = 0.359\n",
      "15500/22300 (epoch 34), train_loss = 1.278, time/batch = 0.354\n",
      "15600/22300 (epoch 34), train_loss = 1.403, time/batch = 0.353\n",
      "15700/22300 (epoch 35), train_loss = 1.422, time/batch = 0.353\n",
      "15800/22300 (epoch 35), train_loss = 1.371, time/batch = 0.354\n",
      "15900/22300 (epoch 35), train_loss = 1.392, time/batch = 0.356\n",
      "16000/22300 (epoch 35), train_loss = 1.300, time/batch = 0.356\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "16100/22300 (epoch 36), train_loss = 1.413, time/batch = 0.359\n",
      "16200/22300 (epoch 36), train_loss = 1.383, time/batch = 0.358\n",
      "16300/22300 (epoch 36), train_loss = 1.452, time/batch = 0.358\n",
      "16400/22300 (epoch 36), train_loss = 1.352, time/batch = 0.356\n",
      "16500/22300 (epoch 36), train_loss = 1.347, time/batch = 0.358\n",
      "16600/22300 (epoch 37), train_loss = 1.455, time/batch = 0.355\n",
      "16700/22300 (epoch 37), train_loss = 1.408, time/batch = 0.352\n",
      "16800/22300 (epoch 37), train_loss = 1.418, time/batch = 0.360\n",
      "16900/22300 (epoch 37), train_loss = 1.353, time/batch = 0.357\n",
      "17000/22300 (epoch 38), train_loss = 1.399, time/batch = 0.354\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "17100/22300 (epoch 38), train_loss = 1.396, time/batch = 0.355\n",
      "17200/22300 (epoch 38), train_loss = 1.507, time/batch = 0.353\n",
      "17300/22300 (epoch 38), train_loss = 1.499, time/batch = 0.360\n",
      "17400/22300 (epoch 39), train_loss = 1.361, time/batch = 0.354\n",
      "17500/22300 (epoch 39), train_loss = 1.420, time/batch = 0.359\n",
      "17600/22300 (epoch 39), train_loss = 1.468, time/batch = 0.359\n",
      "17700/22300 (epoch 39), train_loss = 1.356, time/batch = 0.355\n",
      "17800/22300 (epoch 39), train_loss = 1.197, time/batch = 0.358\n",
      "17900/22300 (epoch 40), train_loss = 1.368, time/batch = 0.358\n",
      "18000/22300 (epoch 40), train_loss = 1.361, time/batch = 0.357\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "18100/22300 (epoch 40), train_loss = 1.467, time/batch = 0.356\n",
      "18200/22300 (epoch 40), train_loss = 1.360, time/batch = 0.356\n",
      "18300/22300 (epoch 41), train_loss = 1.346, time/batch = 0.353\n",
      "18400/22300 (epoch 41), train_loss = 1.377, time/batch = 0.359\n",
      "18500/22300 (epoch 41), train_loss = 1.363, time/batch = 0.355\n",
      "18600/22300 (epoch 41), train_loss = 1.319, time/batch = 0.362\n",
      "18700/22300 (epoch 41), train_loss = 1.366, time/batch = 0.356\n",
      "18800/22300 (epoch 42), train_loss = 1.336, time/batch = 0.355\n",
      "18900/22300 (epoch 42), train_loss = 1.417, time/batch = 0.353\n",
      "19000/22300 (epoch 42), train_loss = 1.347, time/batch = 0.353\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "19100/22300 (epoch 42), train_loss = 1.286, time/batch = 0.358\n",
      "19200/22300 (epoch 43), train_loss = 1.392, time/batch = 0.360\n",
      "19300/22300 (epoch 43), train_loss = 1.341, time/batch = 0.354\n",
      "19400/22300 (epoch 43), train_loss = 1.365, time/batch = 0.355\n",
      "19500/22300 (epoch 43), train_loss = 1.416, time/batch = 0.354\n",
      "19600/22300 (epoch 43), train_loss = 1.303, time/batch = 0.359\n",
      "19700/22300 (epoch 44), train_loss = 1.251, time/batch = 0.357\n",
      "19800/22300 (epoch 44), train_loss = 1.428, time/batch = 0.353\n",
      "19900/22300 (epoch 44), train_loss = 1.328, time/batch = 0.357\n",
      "20000/22300 (epoch 44), train_loss = 1.305, time/batch = 0.352\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "20100/22300 (epoch 45), train_loss = 1.376, time/batch = 0.354\n",
      "20200/22300 (epoch 45), train_loss = 1.394, time/batch = 0.355\n",
      "20300/22300 (epoch 45), train_loss = 1.395, time/batch = 0.359\n",
      "20400/22300 (epoch 45), train_loss = 1.307, time/batch = 0.359\n",
      "20500/22300 (epoch 45), train_loss = 1.357, time/batch = 0.360\n",
      "20600/22300 (epoch 46), train_loss = 1.350, time/batch = 0.353\n",
      "20700/22300 (epoch 46), train_loss = 1.367, time/batch = 0.356\n",
      "20800/22300 (epoch 46), train_loss = 1.449, time/batch = 0.354\n",
      "20900/22300 (epoch 46), train_loss = 1.344, time/batch = 0.352\n",
      "21000/22300 (epoch 47), train_loss = 1.401, time/batch = 0.358\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "21100/22300 (epoch 47), train_loss = 1.348, time/batch = 0.362\n",
      "21200/22300 (epoch 47), train_loss = 1.352, time/batch = 0.354\n",
      "21300/22300 (epoch 47), train_loss = 1.340, time/batch = 0.355\n",
      "21400/22300 (epoch 47), train_loss = 1.333, time/batch = 0.356\n",
      "21500/22300 (epoch 48), train_loss = 1.337, time/batch = 0.360\n",
      "21600/22300 (epoch 48), train_loss = 1.330, time/batch = 0.350\n",
      "21700/22300 (epoch 48), train_loss = 1.400, time/batch = 0.357\n",
      "21800/22300 (epoch 48), train_loss = 1.268, time/batch = 0.355\n",
      "21900/22300 (epoch 49), train_loss = 1.390, time/batch = 0.356\n",
      "22000/22300 (epoch 49), train_loss = 1.374, time/batch = 0.357\n",
      "model saved to models_char_rnn/model.ckpt\n",
      "22100/22300 (epoch 49), train_loss = 1.383, time/batch = 0.353\n",
      "22200/22300 (epoch 49), train_loss = 1.388, time/batch = 0.351\n",
      "model saved to models_char_rnn/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# let's train!\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[00mchars_vocab.pkl\u001b[0m  \u001b[00mconfig.pkl\u001b[0m                            \u001b[00mmodel.ckpt-22299.index\u001b[0m\n",
      "\u001b[00mcheckpoint\u001b[0m       \u001b[00mmodel.ckpt-22299.data-00000-of-00001\u001b[0m  \u001b[00mmodel.ckpt-22299.meta\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# we're done with the model. the weights are now safe inside our storage\n",
    "% ls {args.save_dir}\n",
    "\n",
    "# so begone all the ops, graphs & variables!\n",
    "# you may ask, why is this line needed? try commenting out the line and see what happens later in the sampling stage\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalulation\n",
    "\n",
    "<font color=red>**Your model could be evaluated without traning procedure,**</font> if you saved and loaded your model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER = PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES = 0\n",
    "\n",
    "# load a bunch of libraries\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "from six import text_type\n",
    "import sys\n",
    "\n",
    "# this module is from the utils.py file of this folder\n",
    "# it handles loading texts to digits (aka. tokens) which are recognizable for the model\n",
    "from utils import TextLoader\n",
    "\n",
    "# this module is from the char_rnn.py file of this folder\n",
    "# the task is implementing the CharRNN inside the class definition from this file\n",
    "from char_rnn import Model\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# sampling definition for evaluation phase\n",
    "# it uses the saved model and spit out some characters from the RNN model\n",
    "def sample_eval(args):\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    #Use most frequent char if no prime is given\n",
    "    if args.prime == '':\n",
    "        args.prime = chars[0]\n",
    "    model = Model(saved_args, training=False)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print(str(model.sample(sess, chars, vocab, args.n, args.prime)),'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(n=500, prime='', save_dir='models_char_rnn')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# argparsing for sampling from the model\n",
    "parser_sample = argparse.ArgumentParser(\n",
    "                   formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser_sample.add_argument('--save_dir', type=str, default='models_char_rnn',\n",
    "                    help='model directory to store checkpointed models')\n",
    "parser_sample.add_argument('-n', type=int, default=500,\n",
    "                    help='number of characters to sample')\n",
    "parser_sample.add_argument('--prime', type=text_type, default=u'',\n",
    "                    help='prime text')\n",
    "sys.argv = ['-f']\n",
    "args_sample = parser_sample.parse_args()\n",
    "args_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models_char_rnn/model.ckpt-22299\n",
      " Messingime, my harmly with some like a crain Kemity.\n",
      "\n",
      "SOCKURD OF:\n",
      "You make will have you wrought our tame time? I hallow;\n",
      "Or all this strange that his jaster to may margaret;\n",
      "Hath hearts of Honine worky bring of these,\n",
      "That have conceal my mother not be at\n",
      "Sped.\n",
      "\n",
      "BENVOLIO:\n",
      "Why, had lessist 'Vount of forcucation.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Awaod! mind friar.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Not an unalisture Tears Aufidius\n",
      "That I fear than Petarance and talk of him.\n",
      "\n",
      "PETRUCHIO:\n",
      "Lay I may benry Merce:\n",
      "I provip, if the cam utf-8\n"
     ]
    }
   ],
   "source": [
    "# let's sample!\n",
    "sample_eval(args_sample)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Briefly summarize what & how you did, and why you did that way.\n",
    "This is also for checking yourself if you really learned something from this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "* Rnn Cell : LSTM\n",
    "* Not used dropout in traning,test time\n",
    "* transform input to embedding (vocab_soze,rnn_size)\n",
    "* Use RNN decoder for the seq2seq model to get characters (rnn cell : LSTM) </BR>\n",
    "     => output size : (batch_size,rnn_size)\n",
    "* Use output layer to classify and get logits   </BR>\n",
    "     => output size : (batch_size,rnn_size) -> logits size : (batch_size,vocab_size)\n",
    "* Use weighted cross-entropy loss for a sequence of logits (tf.contrib.legacy_seq2seq.sequence_loss_by_example)\n",
    "* softmax function for probability outputs\n",
    "\n",
    "## Hyperparameter\n",
    "* learning_rate : 0.002\n",
    "* decay_rate : 0.97\n",
    "* Cell : LSTM \n",
    "* rnn_size : 128\n",
    "* num_layers : 5\n",
    "* epoch : 50\n",
    "* input_keep_prob=1.0\n",
    "* output_keep_prob=1.0\n",
    "* optimizer : Adam \n",
    "\n",
    "## Sample\n",
    "1. Move the state to the last character state in the prime.\n",
    "```python\n",
    "    state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "    for char in prime[:-1]:\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {self.input_data: x, self.initial_state: state}\n",
    "        [state] = sess.run([self.final_state], feed)\n",
    "```\n",
    "\n",
    "2. let x be input that last character in the prime.\n",
    "3. get probabilities and final state by x and state.\n",
    "```python\n",
    "    ret = prime\n",
    "    char = prime[-1]\n",
    "    for _ in range(num):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "        feed = {self.input_data: x, self.initial_state: state}\n",
    "        [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "        p = probs[0]\n",
    "```\n",
    "4. pick character by value taken from the standard normal distribution\n",
    "5. add a character to return value and assign a predict character to input.\n",
    "```python\n",
    "        sample = weighted_pick(p)\n",
    "\n",
    "        pred = chars[sample]\n",
    "        ret += pred\n",
    "        char = pred\n",
    "    return ret      \n",
    "\n",
    "    def weighted_pick(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. there are other intentional pitfalls inside the code . just copy-pasting the __init__() will not work. can you tell me why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단순히 init()함수를 copy-pasting하면 char_rnn.py에서 from tensorflow.contrib import legacy_seq2seq\n",
    "를 하지 않아 에러가 뜬다.\n",
    "\n",
    "그 외에는 따로 코드를 고치지 않아도 실행은 되지만 training loss가 같은 값으로 유지되며 학습이 잘 되지 않는 것을 볼 수 있었다. 그래서 기존의 learning_rate와 decay_rate를 바꾸어가며 실험을 하였더니 어느정도 원하는 결과를 얻을 수 있었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

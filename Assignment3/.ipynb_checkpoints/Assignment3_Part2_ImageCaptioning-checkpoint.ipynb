{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 2: Image Captioning with RNNs\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sunyoung Kwon, September 2017\n",
    "\n",
    "Now, you're going to leave behind your implementations and instead migrate to one of popular deep learning frameworks, **TensorFlow**. <br>\n",
    "In this notebook, <font color=red>**you should use tensorflow RNN libraries**</font> for image captioning of the Microsoft COCO dataset. <br>\n",
    "\n",
    "For this exercise we will use the 2014 release of the **[Microsoft COCO dataset](http://mscoco.org/)** which has become the standard testbed for image captioning. The dataset consists of about 80,000 images, each annotated with 5 captions written by workers on Amazon Mechanical Turk.\n",
    "\n",
    "The raw images takes up so much (nearly 20GB), we will use the preprocessed COCO datasets which are extracted features from the VGG-16 network and reduced the dimensionality of the features from 4096 to 512 by standford CS231N lecture.\n",
    "\n",
    "To download the data, change to the `coco/` directory and run the script `get_coco_data.sh`.\n",
    "- caption and image index: `train2014_captions.h5`\n",
    "- idx_to_word, word_to_idx: `coco2014_vocab.json`\n",
    "- extracted and reduced image features: `train2014_vgg16_fc7_pca.h5`\n",
    "- URLs of the images for visualization: `train2014_urls.txt`\n",
    "  <br>(Since images are downloaded on-the-fly, **you must be connected to the internet to view images**)\n",
    "\n",
    "The file `coco_utils.py` has utilities for coco datasets from load COCO data to visualization and evaluation.\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1, 2 & 3**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Some helpful tutorials and references for assignment #3:\n",
    "- [1] TensorFlow official tutorials. [[link]](https://www.tensorflow.org/get_started/get_started)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] Microsoft COCO (http://mscoco.org/dataset/#overview)\n",
    "- [4] Microsoft COCO Captions (https://arxiv.org/pdf/1504.00325.pdf)\n",
    "- [5] Vinyals, Oriol, et al. \"Show and tell: Lessons learned from the 2015 mscoco image captioning challenge.\" IEEE transactions on pattern analysis and machine intelligence 39.4 (2017): 652-663.\n",
    "\n",
    "\n",
    "\n",
    "## Image Captioning (40 points)\n",
    "\n",
    "- input : extracted image features from the VGG-16 network<br>\n",
    "- output: predicted captions<br>\n",
    "- evaluation: average BLEU scores for validation and independent test dataset\n",
    "- data: download by run the `./coco/get_coco_data.sh`\n",
    "- model: save your captiong model in model_path (`./models_captioning`)\n",
    "- You muse use TensorFlow RNN module (such as tf.contrib.rnn)\n",
    "\n",
    "Example of image captioning \n",
    "\n",
    "<img src=\"./files/showandtell.png\" width=70%>\n",
    "[Source: \"Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries\n",
    "The preprocessed COCO datasets and libraries are loaded.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "from coco_utils import *\n",
    "from captioning import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "model_path='./models_captioning'\n",
    "data_path ='./coco/coco_captioning'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO data from disk; this returns a dictionary\n",
    "# We'll work with dimensionality-reduced features for this notebook\n",
    "\n",
    "data = load_coco_data(base_dir=data_path)\n",
    "if len(data)==8 : \n",
    "    print('COCO data load complete')\n",
    "   \n",
    "for k, v in data.items():\n",
    "    if type(v) == np.ndarray:\n",
    "        print(k, type(v), v.shape, v.dtype)\n",
    "    else:\n",
    "        print(k, type(v), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample images and captions\n",
    "captions, features, urls = sample_coco_minibatch(data, batch_size=1)\n",
    "show_samples(captions, urls, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for training\n",
    "\n",
    "For simple test, you can load subsample of total train data.<br>\n",
    "But you should use <font color=red>**full train_data**</font> for final test.<br>\n",
    "You will be able to verify your captioning model more quickly with the small train data.<br>\n",
    "<font color=red>**You must show loss changes with full train data**</font> when you train. <br>\n",
    "And you must show maxlen, n_words, captions.shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------\n",
    "# CAUTION: Do not change maxlen(17), n_words(1004), input_dimension(512)\n",
    "# you should use 512 extracted image features\n",
    "# Do not change coco_utils.py\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "#----------------------------------------------\n",
    "# [case 1] subsample\n",
    "# Try to subsamples of train_data for simple test\n",
    "#----------------------------------------------\n",
    "train_data = load_coco_data(base_dir=data_path, max_train=500)\n",
    "\n",
    "#----------------------------------------------\n",
    "# [case 2] full train data\n",
    "# Try to full train_data for real test\n",
    "#----------------------------------------------\n",
    "#train_data = data\n",
    "\n",
    "captions = train_data['train_captions']\n",
    "img_idx  = train_data['train_image_idxs']\n",
    "img_features = train_data['features'][img_idx]\n",
    "word_to_idx = train_data['word_to_idx']\n",
    "idx_to_word = train_data['idx_to_word']\n",
    "\n",
    "n_words = len(word_to_idx)\n",
    "maxlen = train_data['train_captions'].shape[1]\n",
    "input_dimension = train_data['features'].shape[1]\n",
    "\n",
    "\n",
    "print(n_words)\n",
    "print(maxlen)\n",
    "print(img_features.shape, captions.shape)\n",
    "\n",
    "vcaptions = train_data['val_captions']\n",
    "vimg_idx  = train_data['val_image_idxs']\n",
    "vimg_features = train_data['features'][vimg_idx]\n",
    "print(vimg_features.shape, vcaptions.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "# TODO: Implement for caption training class in captioning.py\n",
    "# you should use TensorFlow RNN libraries (such as tf.contrib.rnn)\n",
    "# You can modify classes, functions, parameters according to your model structure.\n",
    "# You can also hide the train function in the Captioning class.\n",
    "# Just show the loss changes depending on the learning procedure.\n",
    "#\n",
    "# class Captioning():\n",
    "#     def __init(self,...):\n",
    "#     def model(self,...):\n",
    "#     def predict(self,...):\n",
    "#################################################################################\n",
    "\n",
    "def train(img_features, captions):\n",
    "    #-------------------------------------------------------------\n",
    "    # You must show maxlen, n_words, caption.shape !!\n",
    "    #-------------------------------------------------------------\n",
    "    print(maxlen, n_words, captions.shape)\n",
    "    #################################################\n",
    "    # TODO: Implement caption training\n",
    "    # - save your trained model in model_path\n",
    "    # - must print about 10 loss changes !!\n",
    "    #################################################\n",
    "\n",
    "    \n",
    "    #print(\"Current Cost: \", loss_value, \"\\t Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    \n",
    "    #################################################\n",
    "    #                END OF YOUR CODE               #\n",
    "    #################################################\n",
    "    \n",
    "train(img_features, captions) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "You can test your model in train, validation samples with sample images.<br>\n",
    "Final evaluation will be done with average BLEU score.<br>\n",
    "BLEU (bilingual evaluation understudy) score is between 0 and 1. The values closer to 1 represents more similar texts. <br>\n",
    "https://en.wikipedia.org/wiki/BLEU\n",
    "<font color=red>**Your model could be evaluated without traning procedure,**</font> if you saved and loaded your model properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time, os, json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "import tensorflow.python.platform\n",
    "\n",
    "from coco_utils import *\n",
    "from captioning import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "model_path='./models_captioning'\n",
    "data_path ='./coco/coco_captioning'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "train_data = load_coco_data()\n",
    "\n",
    "captions = train_data['train_captions']\n",
    "img_idx  = train_data['train_image_idxs']\n",
    "img_features = train_data['features'][img_idx]\n",
    "word_to_idx = train_data['word_to_idx']\n",
    "idx_to_word = train_data['idx_to_word']\n",
    "vcaptions = train_data['val_captions']\n",
    "vimg_idx  = train_data['val_image_idxs']\n",
    "vimg_features = train_data['features'][vimg_idx]\n",
    "\n",
    "n_words = len(word_to_idx)\n",
    "maxlen = train_data['train_captions'].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for image captioning predicting\n",
    "# The image_captioning should include following functions\n",
    "# - load your saved model \n",
    "# - return predicted captions from image features\n",
    "\n",
    "def image_captioning(features) :\n",
    "    pr_captions = np.zeros((features.shape[0],maxlen),int)\n",
    "    #################################################\n",
    "    # TODO: Implement predicting image captioning\n",
    "    # - load your saved model\n",
    "    # - predict the captions\n",
    "    #################################################\n",
    "    \n",
    "    #################################################\n",
    "    #                END OF YOUR CODE               #\n",
    "    #################################################\n",
    "    return pr_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'val']:\n",
    "    minibatch = sample_coco_minibatch(train_data, split=split, batch_size=1)\n",
    "\n",
    "    gt_captions, features, urls = minibatch\n",
    "    pr_captions = image_captioning(features)\n",
    "\n",
    "    show_predict_samples(gt_captions, pr_captions, urls, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints unigram BLEU score averaged over val dataset\n",
    "\n",
    "def evaluate_model(data, split):\n",
    "    BLEUscores = {}\n",
    "\n",
    "    minibatch = sample_coco_minibatch(data, split=split, batch_size=\"All\")\n",
    "    gt_captions, features, urls = minibatch\n",
    "    gt_captions = decode_captions(gt_captions, data['idx_to_word'])\n",
    "\n",
    "    pr_captions = image_captioning(features)\n",
    "    pr_captions = decode_captions(pr_captions, data['idx_to_word'])\n",
    "\n",
    "    total_score = 0.0\n",
    "        \n",
    "    for gt_caption, pr_caption, url in zip(gt_captions, pr_captions, urls):\n",
    "        total_score += BLEU_score(gt_caption, pr_caption)\n",
    "\n",
    "    BLEUscores[split] = total_score / len(pr_captions)\n",
    "\n",
    "    for split in BLEUscores:\n",
    "        print('Average BLEU score for %s: %f' % (split, BLEUscores[split]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(train_data,'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for TA, TA will evaluate your model with independent test_data\n",
    "# the number of captions of test_data is about 200,000\n",
    "# you should handle captions up to 200,000\n",
    "\n",
    "#evaluate_model(test_data,'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe what you did here\n",
    "In this cell you should also write an explanation of what you did.<br>\n",
    "(A detailed description of your model, structure, tensorflow module and others)\n",
    "<font color=red>**You must describe your model**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tell us here_"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

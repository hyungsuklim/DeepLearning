{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 1: Implementing Recurrent Neural Networks\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Sunyoung Kwon, September 2017\n",
    "\n",
    "In this notebook, you will learn how to implement recurrent neural netowrks (RNNs) <font color=red>**without using deep learning frameworks**</font>. <br>\n",
    "The goal here is to get better understanding of RNNs before using the **TensorFlow** deep learning framework. <br>\n",
    "<img src=\"./files/rnn.PNG\">\n",
    "<img src=\"./files/unfold.jpg\" width=80%>\n",
    "[A recurrent neural network and the unfolding in time of the computation involved in its forward computation. Source: Nature]<br>\n",
    "$x_t$: input at time step $t$<br>\n",
    "$s_t$: hidden state at time step $t$<br>\n",
    "$o_t$: output at time step $t$<br>\n",
    "$W, U, V$: weight for input, hidden state, and output<br>\n",
    "$s_t = tanh(Ux_t + Ws_{t-1})$<br>\n",
    "$o_t = softmax(Vs_t)$<br>\n",
    "\n",
    "\n",
    "There are **4 sections**, and in each section, you need to follow the instructions to complete the skeleton codes and explain them.\n",
    "\n",
    "#### Implementing Vanilla RNN ( 20 points )\n",
    "1. [Single timestep forward](#1) ( 5 points )\n",
    "2. [Single timestep backward](#2) ( 5 points )\n",
    "3. [forward pass for an entire sequence](#3) ( 5 points )\n",
    "4. [backward pass for an entire sequence](#4) ( 5 points )\n",
    "\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1, 2 & 3**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Some helpful tutorials and references for assignment #3:\n",
    "- [1] TensorFlow official tutorials. [[link]](https://www.tensorflow.org/get_started/get_started)\n",
    "- [2] Stanford CS231n lectures. [[link]](http://cs231n.stanford.edu/)\n",
    "- [3] http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "- [4] https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries\n",
    "The libraries requared for RNNs are loaded. You will need to implement the functions in *rnn_layers.py*. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from rnn_layers import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\"></a> 1,2. Vanilla RNN: step forward and backward ( 10 points )\n",
    "\n",
    "In this section, you will implement step forward and backward passes for **a single timestep** of a vanilla recurrent neural network. Using the code provided as guidance, complete the functions `rnn_step_forward` and `rnn_step_backward` in *rnn_layers.py* file. just write the code in whatever way you find most clear.\n",
    "\n",
    "When you are done, run the following to check your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1\"></a> 1. single timestep forward ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement rnn_step_forward\n",
    "# errors should be less than 1e-8\n",
    "x, prev_h, Wx, Wh, b, expected_next_h = getdata_rnn_step_forward()\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1\"></a> 2. single timestep backward ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement rnn_step_backward\n",
    "# errors should be less than 1e-8\n",
    "np.random.seed(2177)\n",
    "\n",
    "x, h, Wx, Wh, b, expected_next_h = getdata_rnn_step_forward()\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "dx_num, dprev_h_num, dWx_num, dWh_num, db_num = getdata_rnn_step_backward(x,h,Wx,Wh,b,dnext_h)\n",
    "\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print('dx error     : ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error    : ', rel_error(dWx_num, dWx))\n",
    "print('dWh error    : ', rel_error(dWh_num, dWh))\n",
    "print('db error     : ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"1\"></a> 3,4. Vanilla RNN: forward and backward ( 10 points )\n",
    "\n",
    "Combining the single timestep forward and backward passes for vanilla RNN, you will implement a vanilla RNN that process **an entire sequence**. Using the code provided as guidance, complete the functions `rnn_forward` and `rnn_backward` in *rnn_layers.py* file. \n",
    "\n",
    "When you are done, run the following to check your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1\"></a> 3. forward pass for an entire sequence ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement rnn_forward\n",
    "# errors should be less than 1e-7\n",
    "x,h0,Wx,Wh,b, expected_h = getdata_rnn_forward()\n",
    "\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"1\"></a> 4. backward pass for an entire sequence ( 5 points )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement rnn_forward\n",
    "# errors should be less than 1e-7\n",
    "np.random.seed(2177)\n",
    "\n",
    "x,h0,Wx,Wh,b, expected_h = getdata_rnn_forward()\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "dout = np.random.randn(*out.shape)\n",
    "dx_num, dh0_num, dWx_num, dWh_num, db_num = getdata_rnn_backward(x,h0,Wx,Wh,b,dout)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

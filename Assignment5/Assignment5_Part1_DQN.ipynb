{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br>Assignment #5 Part 1: Implementing and Training a Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Hyemi Jang, November 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement one of famous reinforcement learning algorithm, Deep Q-Network (DQN) of DeepMind. <br>\n",
    "The goal here is to understand a basic form of DQN [1, 2] and learn how to use OpenAI Gym toolkit [3].<br>\n",
    "You need to follow the instructions to implement the given classes.\n",
    "\n",
    "1. [Play](#play) ( 50 points )\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **two parts of the assignment**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; Team_#)\n",
    "\n",
    "### Some helpful references for assignment #4 :\n",
    "- [1] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013). [[pdf]](https://www.google.co.kr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiI3aqPjavVAhXBkJQKHZsIDpgQFgg7MAI&url=https%3A%2F%2Fwww.cs.toronto.edu%2F~vmnih%2Fdocs%2Fdqn.pdf&usg=AFQjCNEd1AJoM72DeDpI_GBoPuv7NnVoFA)\n",
    "- [2] Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529-533. [[pdf]](https://www.nature.com/nature/journal/v518/n7540/pdf/nature14236.pdf)\n",
    "- [3] OpenAI GYM website [[link]](https://gym.openai.com/envs) and [[git]](https://github.com/openai/gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. OpenAI Gym\n",
    "\n",
    "OpenAI Gym is a toolkit to support diverse environments for developing reinforcement learning algorithms. You can use the toolkit with Python as well as TensorFlow. Installation guide of OpenAI Gym is offered by [this link](https://github.com/openai/gym#installation) or just type the command \"pip install gym\" (as well as \"pip install gym[atari]\" for Part2). \n",
    "\n",
    "After you set up OpenAI Gym, you can use APIs of the toolkit by inserting <font color=red>import gym</font> into your code. In this assignment, you must build one of famous reinforcement learning algorithms whose agent can run on OpenAI Gym environments. Please check how to use APIs such as funcions interacting with environments in the followings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2 \n",
    "import gym\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# Make an environment instance of CartPole-v0.\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Before interacting with the environment and starting a new episode, you must reset the environment's state.\n",
    "state = env.reset()\n",
    "\n",
    "# Uncomment to show the screenshot of the environment (rendering game screens)\n",
    "# env.render() \n",
    "\n",
    "# You can check action space and state (observation) space.\n",
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "print(num_actions)\n",
    "print(state_shape)\n",
    "\n",
    "# \"step\" function performs agent's actions given current state of the environment and returns several values.\n",
    "# Input: action (numerical data)\n",
    "#        - env.action_space.sample(): select a random action among possible actions.\n",
    "# Output: next_state (numerical data, next state of the environment after performing given action)\n",
    "#         reward (numerical data, reward of given action given current state)\n",
    "#         terminal (boolean data, True means the agent is done in the environment)\n",
    "next_state, reward, terminal, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a DQN agent\n",
    "## 1) Overview of implementation in the notebook\n",
    "\n",
    "The assignment is based on a method named by Deep Q-Network (DQN) [1,2]. You could find the details of DQN in the papers. The followings show briefly architecture of DQN and its training computation flow.\n",
    "\n",
    "- (Pink flow) Play an episode and save transition records of the episode into a replay memory.\n",
    "- (Green flow) Train DQN so that a loss function in the figure is minimized. The loss function is computed using main Q-network and Target Q-network. Target Q-network needs to be periodically updated by copying the main Q-network.\n",
    "- (Purple flow) Gradient can be autonomously computed by tensorflow engine, if you build a proper optimizer.\n",
    "\n",
    "![](image/architecture.png)\n",
    "\n",
    "There are major 4 components, each of which needs to be implemented in this notebook. The Agent class must have an instance(s) of each class (Environment, DQN, ReplayMemory).\n",
    "- Environment\n",
    "- DQN \n",
    "- ReplayMemory\n",
    "- Agent\n",
    "\n",
    "![](image/components.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Design classes\n",
    "\n",
    "In the code cells, there are only names of functions which are used in TA's implementation and their brief explanations. <font color='green'>...</font> means that the functions need more arguments and <font color='green'>pass</font> means that you need to write more codes. The functions may be helpful when you do not know how to start the assignment. Of course, you could change the functions such as deleting/adding functions or extending/reducing roles of the classes, <font color='red'> just keeping the existence of the classes</font>.\n",
    "\n",
    "### Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, args):\n",
    "        #Initializing environments with arguments from the args.\n",
    "        self.args = args\n",
    "        self.env = env\n",
    "        self.num_actions = num_actions\n",
    "        self.state_shape = list(state_shape)\n",
    "        \n",
    "    def random_action(self):\n",
    "        # Return a random action.\n",
    "        return self.env.action_space.sample()\n",
    "    \n",
    "    def render_worker(self):\n",
    "        # If display in your option is true, do rendering. Otherwise, do not.\n",
    "        #Defining display argument as True/False from self.args.display\n",
    "        if self.args.display:\n",
    "            self.env.render()\n",
    "            sleep(self.args.display_interval)\n",
    "    \n",
    "    def new_episode(self):\n",
    "        # Start a new episode and return the first state of the new episode.\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def act(self, action):\n",
    "        # Perform an action which is given by input argument and return the results of acting.\n",
    "        #As it shows above code, we followed the code and exclude the self.info, because self.info has nothing important.\n",
    "        self.state, self.reward, self.terminal, self.info = self.env.step(action)\n",
    "        self.render_worker()\n",
    "        return self.state, self.reward, self.terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayMemory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, args, state_shape):\n",
    "        self.args = args\n",
    "        self.state_shape = state_shape\n",
    "        #Initialize count and current variable to count the index of action, reward, and termianls.\n",
    "        self.count = 0\n",
    "        self.current = 0\n",
    "        \n",
    "        #Initializing array shape of actions(uint8), rewards(float32), terminals(float32), and states (float32)\n",
    "        #to check which index of each has what values.\n",
    "        self.actions = np.empty(self.args.memory_size, dtype=np.uint8)\n",
    "        self.rewards = np.empty(self.args.memory_size, dtype=np.float32)\n",
    "        self.terminals = np.empty(self.args.memory_size, dtype=np.bool)\n",
    "        self.next_states = np.empty([self.args.memory_size] + self.state_shape, dtype=np.float32)\n",
    "        self.prestates = np.empty([self.args.batch_size] + self.state_shape, dtype=np.float32)\n",
    "        self.poststates = np.empty([self.args.batch_size] + self.state_shape, dtype=np.float32)\n",
    "        \n",
    "    def add(self, action, reward, terminal, next_state):\n",
    "        # Add current_state, action, reward, terminal, (next_state which can be added by your choice). \n",
    "        self.actions[self.current] = action\n",
    "        self.rewards[self.current] = reward\n",
    "        self.terminals[self.current] = terminal\n",
    "        \n",
    "        self.next_states[self.current] = next_state\n",
    "        self.count = max(self.count, self.current+1)\n",
    "        self.current = (self.current+1) % self.args.memory_size\n",
    "    \n",
    "    def mini_batch(self):\n",
    "        # Return a mini_batch whose data are selected according to your sampling method. (such as uniform-random sampling in DQN papers)\n",
    "        batch_idx = []\n",
    "        while len(batch_idx) < self.args.batch_size:\n",
    "            while True:\n",
    "                idx = np.random.randint(low=1, high=self.count)\n",
    "                if idx == self.current:\n",
    "                    continue\n",
    "                if self.terminals[idx-1]:\n",
    "                    continue\n",
    "                break\n",
    "            self.prestates[len(batch_idx)] = self.next_states[idx-1]\n",
    "            self.poststates[len(batch_idx)] = self.next_states[idx]\n",
    "            batch_idx.append(idx)\n",
    "            \n",
    "        actions = self.actions[batch_idx]\n",
    "        rewards = self.rewards[batch_idx]\n",
    "        terminals = self.rewards[batch_idx]\n",
    "        #Returns previous states, actions, rewards, terminals, and post state.\n",
    "        return self.prestates, actions, rewards, terminals, self.poststates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self, args, sess, memory, environment):\n",
    "        self.args = args\n",
    "        self.sess = sess\n",
    "        self.memory = memory\n",
    "        self.env = environment\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.input_shape = self.memory.state_shape\n",
    "        \n",
    "        self.states = tf.placeholder(tf.float32, [None] + self.input_shape)\n",
    "        self.actions = tf.placeholder(tf.uint8, [None])\n",
    "        self.rewards = tf.placeholder(tf.float32, [None])\n",
    "        self.terminals = tf.placeholder(tf.float32, [None])\n",
    "        self.max_q = tf.placeholder(tf.float32, [None])\n",
    "        \n",
    "        self.prediction_Q = self.build_network('pred')\n",
    "        self.target_Q = self.build_network('target')\n",
    "        self.loss, self.optimizer = self.build_optimizer()\n",
    "    \n",
    "    def build_network(self, name):\n",
    "        # Make your a deep neural network\n",
    "        #Tried with convolutional network but didn't give us good rewards.\n",
    "        '''\n",
    "        def conv2d(x, output_dim, kernel_size, stride, initializer, activation, padding='VALID', name='conv2d'):\n",
    "            with tf.variable_scope(name):\n",
    "                stride = [1, 1, stride[0], stride[1]]\n",
    "                kernel = [kernel_size[0], kernel_size[1], x.get_shape()[1], output_dim]\n",
    "\n",
    "                w = tf.get_variable('w', kernel, tf.float32, initializer=initializer)\n",
    "                b = tf.get_variable('b', [output_dim], \n",
    "                        initializer=tf.constant_initializer(0.0))\n",
    "                conv = tf.nn.conv2d(x, w, stride, padding, data_format='NCHW')\n",
    "                out = activation(tf.nn.bias_add(conv, b, 'NCHW'))\n",
    "\n",
    "                return out\n",
    "        '''    \n",
    "        #This is a function for layers with weights and bias matmul operation  \n",
    "        '''\n",
    "        def linear(x, output_size, stddev=0.02, bias_start=0.0, activation=None, name='linear'):\n",
    "            shape = x.get_shape().as_list()\n",
    "            \n",
    "            with tf.variable_scope(name):\n",
    "                w = tf.get_variable('w', [shape[1], output_size], tf.float32, tf.random_normal_initializer(stddev=stddev))\n",
    "                b = tf.get_variable('b', [output_size], initializer=tf.constant_initializer(bias_start))\n",
    "                out = tf.nn.bias_add(tf.matmul(x, w), b)\n",
    "                if activation != None:\n",
    "                    out = activation(out)\n",
    "\n",
    "                return out\n",
    "         '''\n",
    "        with tf.variable_scope(name):\n",
    "            #Tried with difference hidden layer, but took long time for training and the results weren't good enough.\n",
    "            \n",
    "            #fc1 = tf.layers.dense(inputs=self.states, units=100, activation=tf.nn.relu, kernel_initializer=self.kernel_initializer)\n",
    "            #fc2 = tf.layers.dense(inputs=fc1, units=50, activation=tf.nn.relu, kernel_initializer=self.kernel_initializer)\n",
    "            #fc3 = tf.layers.dense(inputs=fc2, units=10, activation=tf.nn.relu, kernel_initializer=self.kernel_initializer)\n",
    "            #Q = tf.layers.dense(inputs=fc3, units=self.num_actions, activation=None, kernel_initializer=self.kernel_initializer)\n",
    "            \n",
    "            #Tried conv layers/linears with matmul method as well, but not good enough results performed.\n",
    "            '''\n",
    "            self.l1 = conv2d(self.states, 32, [8, 8], [4, 4], initializer, activation_fn, name='l1')\n",
    "            self.l2 = conv2d(self.l1, 64, [4, 4], [2, 2], initializer, activation_fn, name='l2')\n",
    "            self.l3 = con2d(self.l2, 64, [3, 3], [1, 1], initializer, activation_fn, name='l3')\n",
    "                \n",
    "            shape = self.l3.get_shape().as_list()\n",
    "            self.l3_flat = tf.reshape(self.l3, [-1, reduce(lambda x, y: x*y, shape[1:])])\n",
    "            self.l4 = linear(self.l3_flat, 512, activation_fn, name='l4')\n",
    "            self.Q = linear(self.l4, self.num_actions, activation_fn, name='Q')\n",
    "            '''\n",
    "            \n",
    "            #Set kernel, bias initializer, and activation function.\n",
    "            self.kernel_initializer = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)\n",
    "            self.bias_initializer = tf.constant_initializer(0.05)\n",
    "            self.activation_fn = tf.nn.relu\n",
    "            \n",
    "            #Created simple Fully_connected layers with small hidden units. This performs the best after configuring params.\n",
    "            #Last layer which is Q being set with softmax function.\n",
    "            fc1 = layers.fully_connected(self.states, 6, biases_initializer=None, activation_fn=self.activation_fn)\n",
    "            fc2 = layers.fully_connected(fc1, 4, biases_initializer=None, activation_fn=self.activation_fn)\n",
    "            Q = layers.fully_connected(fc2, self.num_actions, biases_initializer=None, activation_fn=tf.nn.softmax)\n",
    "            \n",
    "            return Q\n",
    "        \n",
    "    def build_optimizer(self):\n",
    "        # Make your optimizer \n",
    "        # Calculating the target Q value (= r + gamma * maxQ(next_state)) which is written in the slide of chpater 15.\n",
    "        target_q = self.rewards + tf.multiply(1-self.terminals, tf.multiply(self.args.discount_factor, self.max_q))\n",
    "        \n",
    "        # Calculating the predicted Q value\n",
    "        action_one_hot = tf.one_hot(indices=self.actions, depth=self.num_actions, on_value=1.0, off_value=0.0)\n",
    "        pred_q = tf.reduce_sum(tf.multiply(self.prediction_Q, action_one_hot), reduction_indices=1)\n",
    "        \n",
    "        # Calculating the loss and make an optimizer as always we did in previous assignments.\n",
    "        loss = tf.reduce_mean(tf.square(pred_q - target_q))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.args.learning_rate).minimize(loss)\n",
    "        \n",
    "        return loss, optimizer\n",
    "    \n",
    "    def train_network(self):\n",
    "        # Train the prediction_Q network using a mini-batch sampled from the replay memory\n",
    "        # Get minibatch values from ReplayMemory(mini_batch function)\n",
    "        minib_prestates, minib_actions, minib_rewards, minib_terminals, minib_poststates = self.memory.mini_batch()\n",
    "        \n",
    "        # Calculating the target Q value (batch)\n",
    "        minib_q_poststates = self.sess.run(self.target_Q, feed_dict={self.states: minib_poststates})\n",
    "        minib_max_q = np.max(minib_q_poststates, axis=1)\n",
    "        \n",
    "        #Running optimizer by feeding dict.\n",
    "        return self.sess.run([self.loss, self.optimizer], feed_dict={self.states: minib_prestates, self.actions: minib_actions, self.rewards: minib_rewards, self.terminals: minib_terminals, self.max_q: minib_max_q})\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        #We separated these codes as another function(update_target_network)\n",
    "        copy_op = []\n",
    "        pred_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='pred')\n",
    "        target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='target')\n",
    "        for pred_var, target_var in zip(pred_vars, target_vars):\n",
    "            copy_op.append(target_var.assign(pred_var.value()))\n",
    "        self.sess.run(copy_op)\n",
    "    \n",
    "    def predict_Q(self, state):\n",
    "        #Predicting Q.\n",
    "        return self.sess.run(self.prediction_Q, feed_dict={self.states: [state]})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # to save and load\n",
    "import time\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, args, sess):\n",
    "        #As this is the Agent class, we need to initialize environment, replay memory, and DQN class.\n",
    "        self.args = args\n",
    "        self.sess = sess\n",
    "        self.env = Environment(args)\n",
    "        self.memory = ReplayMemory(args, self.env.state_shape)\n",
    "        self.dqn = DQN(self.args, self.sess, self.memory, self.env)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Synchronize the target network with the main network\n",
    "        self.dqn.update_target_network()\n",
    "        \n",
    "        # Seed of the random number generator\n",
    "        np.random.seed(int(time.time()))\n",
    "    \n",
    "    def select_action(self):\n",
    "        # Select an action according ε-greedy. You need to use a random-number generating function and add a library if necessary.\n",
    "        #If train is true, then use self.eps as max of below code. OR use eps_test value from arguments.\n",
    "        if self.args.train:\n",
    "            self.eps = np.max([self.args.eps_min, self.args.eps_init - (self.args.eps_init - self.args.eps_min)*(float(self.step)/float(self.args.max_exploration_step))])\n",
    "        else:\n",
    "            self.eps = self.args.eps_test\n",
    "        #Picking random actions.\n",
    "        if np.random.rand() < self.eps:\n",
    "            action = self.env.random_action()\n",
    "        else:\n",
    "            q = self.dqn.predict_Q(self.state)[0]\n",
    "            action = np.argmax(q)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def train(self):\n",
    "        # Train your agent which has the neural nets.\n",
    "        # Several hyper-parameters are determined by your choice (Options class in the below cell)\n",
    "        # Keep epsilon-greedy action selection in your mind \n",
    "        episodes_count = 0\n",
    "        best_reward = 0\n",
    "        best_avg_reward = 0\n",
    "        episode_reward = 0\n",
    "        episode_rewards = []\n",
    "        \n",
    "        print('----Creating Random Memory----')\n",
    "        self.state = self.env.new_episode()\n",
    "        for self.step in range(1, self.args.max_step + 1):\n",
    "            if self.step == 1:\n",
    "                print('----Trying to upate the network----')\n",
    "            \n",
    "            action = self.select_action() # Select an action by epsilon-greedy\n",
    "            next_state, reward, terminal = self.env.act(action) # Perform the action and receive information of the environment\n",
    "            self.memory.add(action, reward, terminal, next_state) # Save the information\n",
    "            self.state = next_state # Update the input state\n",
    "            \n",
    "            #Adding reward to episode_reward.\n",
    "            episode_reward += reward\n",
    "            if terminal:\n",
    "                episodes_count += 1\n",
    "                episode_rewards.append(episode_reward)\n",
    "                if episode_reward > best_reward:\n",
    "                    best_reward = episode_reward\n",
    "                episode_reward = 0\n",
    "                self.state = self.env.new_episode()\n",
    "            \n",
    "            # Periodically update main network and target network\n",
    "            if self.step >= self.args.training_start_step:\n",
    "                #Updating target_network when self.step is divided by self.copy_interval that we initialized \n",
    "                if self.step % self.args.copy_interval == 0:\n",
    "                    self.dqn.update_target_network()\n",
    "                #Training DQN when the remainder is 0.\n",
    "                if self.step % self.args.train_interval == 0:\n",
    "                    loss, _ = self.dqn.train_network()\n",
    "                if self.step % self.args.show_interval == 0:\n",
    "                    #Calculating min, avg, max rewards from episode.\n",
    "                    max_r = np.max(episode_rewards)\n",
    "                    min_r = np.min(episode_rewards)\n",
    "                    avg_r = np.mean(episode_rewards)\n",
    "                    #Make max reward to best reward\n",
    "                    if max_r > best_reward:\n",
    "                        best_reward = max_r\n",
    "                    if avg_r > best_avg_reward:\n",
    "                        best_avg_reward = avg_r\n",
    "                        #Save the checkpoint using save() function\n",
    "                        self.save()\n",
    "                    print('[%7d/%7d step] avg_r: %.4f, max_r: %3d, min_r: %3d, Best reward: %3d' %(self.step, self.args.max_step, avg_r, max_r, min_r, best_reward))\n",
    "                    episode_rewards = []\n",
    "    \n",
    "    def play(self, num_episode=5, load=True):\n",
    "        # Test your agent \n",
    "        # When performing test, you can show the environment's screen by rendering,\n",
    "        #For each episode, we keep adding rewards until terminal is True. \n",
    "        best_reward = 0\n",
    "        for episode in range(num_episode):\n",
    "            self.state = self.env.new_episode()\n",
    "            current_reward = 0\n",
    "            \n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                action = self.select_action()\n",
    "                next_state, reward, terminal = self.env.act(action)\n",
    "                current_reward += reward\n",
    "                self.state = next_state\n",
    "                \n",
    "                if terminal:\n",
    "                    break\n",
    "            \n",
    "            if current_reward > best_reward:\n",
    "                best_reward = current_reward\n",
    "        \n",
    "        return best_reward\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint_dir = 'cartpole'\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, 'trained_agent'))\n",
    "        \n",
    "    def load(self):\n",
    "        print('Loading checkpoint...!')\n",
    "        checkpoint_dir = 'cartpole'\n",
    "        checkpoint_state = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        self.saver.restore(self.sess, os.path.join(checkpoint_dir, 'trained_agent'))\n",
    "        print('Success to load checkpoint')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train your agent \n",
    "\n",
    "Now, you train an agent to play CartPole-v0. Options class is the collection of hyper-parameters that you can choice. Usage of Options class is not mandatory.<br>\n",
    "The maximum value of total reward which can be aquired from one episode is 200. \n",
    "<font color='red'>**You should show learning status such as the number of observed states and mean/max/min of rewards frequently (for instance, every 100 states).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "\n",
    "\"\"\"\n",
    "You can add more arguments.\n",
    "for example, visualize, memory_size, batch_size, discount_factor, eps_max, eps_min, learning_rate, train_interval, copy_interval and so on\n",
    "\"\"\"\n",
    "#Changed argparse with easydict. We don't know why but it gives us error while we complies it.\n",
    "#So we find easydict method for alternative way\n",
    "args = easydict.EasyDict({\n",
    "    \"env-name\" : \"CartPole-v0\",\n",
    "    \"train\" : True,\n",
    "    \"display\" : False,\n",
    "    \n",
    "    \"max_step\" : 100000,\n",
    "    \"max_exploration_step\" : 10000,\n",
    "    \"memory_size\" : 10000,\n",
    "    \"batch_size\" : 32,\n",
    "    \"num_skipping_states\" : 4,\n",
    "    \"state_length\" : 4,\n",
    "    \n",
    "    \"discount_factor\" : 0.99,\n",
    "    \"eps_init\" : 1.0,\n",
    "    \"eps_min\" : 0.1,\n",
    "    \"eps_test\" : 0.05,\n",
    "    \"learning_rate\" : 1e-5,\n",
    "    \n",
    "    \"training_start_step\" : 100,\n",
    "    \"train_interval\" : 1,\n",
    "    \"copy_interval\" : 100,\n",
    "    \"show_interval\" : 2000,\n",
    "    \"display_interval\" : 0.05,\n",
    "    \n",
    "    \"gpu_num\" : 0\n",
    "})\n",
    "\n",
    "# Basic DQN uses just one GPU.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_num)\n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = False\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Creating Random Memory----\n",
      "----Trying to upate the network----\n",
      "[   2000/ 100000 step] avg_r: 21.2553, max_r:  66, min_r:  10, Best reward:  66\n",
      "[   4000/ 100000 step] avg_r: 18.7009, max_r:  87, min_r:   8, Best reward:  87\n",
      "[   6000/ 100000 step] avg_r: 14.6765, max_r:  44, min_r:   8, Best reward:  87\n",
      "[   8000/ 100000 step] avg_r: 12.4161, max_r:  22, min_r:   8, Best reward:  87\n",
      "[  10000/ 100000 step] avg_r: 10.6543, max_r:  20, min_r:   8, Best reward:  87\n",
      "[  12000/ 100000 step] avg_r: 9.8276, max_r:  16, min_r:   8, Best reward:  87\n",
      "[  14000/ 100000 step] avg_r: 9.9851, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  16000/ 100000 step] avg_r: 9.7745, max_r:  17, min_r:   8, Best reward:  87\n",
      "[  18000/ 100000 step] avg_r: 9.8235, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  20000/ 100000 step] avg_r: 9.8713, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  22000/ 100000 step] avg_r: 9.9406, max_r:  16, min_r:   8, Best reward:  87\n",
      "[  24000/ 100000 step] avg_r: 9.9303, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  26000/ 100000 step] avg_r: 9.7794, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  28000/ 100000 step] avg_r: 9.9950, max_r:  19, min_r:   8, Best reward:  87\n",
      "[  30000/ 100000 step] avg_r: 9.9204, max_r:  16, min_r:   8, Best reward:  87\n",
      "[  32000/ 100000 step] avg_r: 9.9010, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  34000/ 100000 step] avg_r: 10.0350, max_r:  16, min_r:   8, Best reward:  87\n",
      "[  36000/ 100000 step] avg_r: 9.9502, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  38000/ 100000 step] avg_r: 9.8762, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  40000/ 100000 step] avg_r: 9.8039, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  42000/ 100000 step] avg_r: 9.8768, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  44000/ 100000 step] avg_r: 9.8473, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  46000/ 100000 step] avg_r: 9.8614, max_r:  16, min_r:   8, Best reward:  87\n",
      "[  48000/ 100000 step] avg_r: 9.9109, max_r:  18, min_r:   8, Best reward:  87\n",
      "[  50000/ 100000 step] avg_r: 10.0000, max_r:  16, min_r:   8, Best reward:  87\n",
      "[  52000/ 100000 step] avg_r: 9.8333, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  54000/ 100000 step] avg_r: 9.9850, max_r:  21, min_r:   8, Best reward:  87\n",
      "[  56000/ 100000 step] avg_r: 9.9502, max_r:  17, min_r:   8, Best reward:  87\n",
      "[  58000/ 100000 step] avg_r: 9.8719, max_r:  13, min_r:   8, Best reward:  87\n",
      "[  60000/ 100000 step] avg_r: 9.7647, max_r:  13, min_r:   8, Best reward:  87\n",
      "[  62000/ 100000 step] avg_r: 9.8382, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  64000/ 100000 step] avg_r: 9.8960, max_r:  18, min_r:   8, Best reward:  87\n",
      "[  66000/ 100000 step] avg_r: 9.9800, max_r:  17, min_r:   8, Best reward:  87\n",
      "[  68000/ 100000 step] avg_r: 10.0000, max_r:  17, min_r:   8, Best reward:  87\n",
      "[  70000/ 100000 step] avg_r: 9.8960, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  72000/ 100000 step] avg_r: 9.9109, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  74000/ 100000 step] avg_r: 9.9353, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  76000/ 100000 step] avg_r: 9.9751, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  78000/ 100000 step] avg_r: 9.7561, max_r:  13, min_r:   8, Best reward:  87\n",
      "[  80000/ 100000 step] avg_r: 9.9800, max_r:  19, min_r:   8, Best reward:  87\n",
      "[  82000/ 100000 step] avg_r: 9.9208, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  84000/ 100000 step] avg_r: 9.8177, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  86000/ 100000 step] avg_r: 9.8186, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  88000/ 100000 step] avg_r: 9.8333, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  90000/ 100000 step] avg_r: 9.8325, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  92000/ 100000 step] avg_r: 9.8719, max_r:  14, min_r:   8, Best reward:  87\n",
      "[  94000/ 100000 step] avg_r: 9.9700, max_r:  16, min_r:   8, Best reward:  87\n",
      "[  96000/ 100000 step] avg_r: 9.8088, max_r:  15, min_r:   8, Best reward:  87\n",
      "[  98000/ 100000 step] avg_r: 9.9701, max_r:  15, min_r:   8, Best reward:  87\n",
      "[ 100000/ 100000 step] avg_r: 9.8571, max_r:  16, min_r:   8, Best reward:  87\n",
      "Loading checkpoint...!\n",
      "INFO:tensorflow:Restoring parameters from cartpole/trained_agent\n",
      "Success to load checkpoint\n",
      "[12.0, 13.0, 14.0, 10.0, 13.0, 11.0, 10.0, 10.0, 12.0, 11.0, 12.0, 12.0, 13.0, 14.0, 10.0, 14.0, 14.0, 15.0, 10.0, 11.0]\n",
      "12.05\n"
     ]
    }
   ],
   "source": [
    "# fot train\n",
    "with tf.Session(config=config) as sess:\n",
    "    myAgent = Agent(args, sess) # It depends on your class implementation\n",
    "    myAgent.train()\n",
    "    # myAgent.save()\n",
    "    \n",
    "    # test\n",
    "    myAgent.load()\n",
    "    rewards = []\n",
    "    for i in range(20):\n",
    "        r = myAgent.play() # play() returns the reward cumulated in one episode\n",
    "        rewards.append(r)\n",
    "    mean = np.mean(rewards)\n",
    "    print(rewards)\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"play\"></a> 3. Test the trained agent ( 15 points )\n",
    "\n",
    "Now, we test your agent and calculate an average reward of 20 episodes.\n",
    "- 0 <= average reward < 50 : you can get 0 points\n",
    "- 50 <= average reward < 100 : you can get 10 points\n",
    "- 100 <= average reward < 190 : you can get 35 points\n",
    "- 190 <= average reward <= 200 : you can get 50 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...!\n",
      "INFO:tensorflow:Restoring parameters from cartpole/trained_agent\n",
      "Success to load checkpoint\n",
      "[200.0, 68.0, 200.0, 200.0, 200.0, 200.0, 200.0, 149.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 157.0, 200.0, 200.0]\n",
      "188.7\n"
     ]
    }
   ],
   "source": [
    "# for test\n",
    "with tf.Session(config=config) as sess:\n",
    "    #args = parser.parse_args() # You set the option of test phase\n",
    "    args[\"train\"] = False\n",
    "    myAgent = Agent(args, sess) # It depends on your class implementation\n",
    "    myAgent.load()\n",
    "    rewards = []\n",
    "    for i in range(20):\n",
    "        r = myAgent.play() # play() returns the reward cumulated in one episode\n",
    "        rewards.append(r)\n",
    "    mean = np.mean(rewards)\n",
    "    print(rewards)\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
